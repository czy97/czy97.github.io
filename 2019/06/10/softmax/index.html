
<!DOCTYPE html>
<html lang="zh-CN,en,default">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Zhengyang Chen&#39;s blog">
    <title>多种Softmax对比 - Zhengyang Chen&#39;s blog</title>
    <meta name="author" content="Zhengyang Chen">
    
        <meta name="keywords" content="hexo,javascript,javascript,hexo">
    
    
        <link rel="icon" href="http://czy97.github.io/assets/images/kelasong.jpeg">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhengyang Chen","sameAs":["https://github.com/"],"image":"kelasong.jpeg"},"articleBody":"损失函数作为优化一个神经网络的目标，对于神经网络最后的性能影响重大。Softmax+crossentropy作为一个分类的损失函数受到大家广泛的使用。本文将对于softmax做一个推导，并对很多论文中对softmax所做的改进做一个总结。\n\n\nSoftmax+CrossEntropy损失函数损失函数的定义softmax交叉熵损失函数主要是针对多类的分类任务使用。假设任务中的类别数是 $C$ 。为了使用softmax交叉熵损失函数，首先需要将输入转为一个 $C$ 维的向量，这里用 $x_i ,(i \\in [1,C])$ 来表示向量中每个位置的值。\n然后，对这个$C$维向量中的所有值做softmax运算，以将每个值转为一种可以作为某种具有概率意义的值,计算之后的值记为 $z_i ,(i \\in [1,C]) $ ,这样所有的$z_i$之和便是1，便可以用来表示输入样本输入每个类的概率值：\n\nz_i =  \\cfrac{e^{x_i}}{\\sum_{j=1}^Ce^{x_j}}最后对 $z_i$ 做negtive log likelihood和crossEntropy运算，最后的损失函数被定义为：\n\nL =  -\\sum^C_{i=1}y_iln(z_i) = -y_iln(z_i)这里的 $y_i$ 输入样本的类标签，如果输入样本输入类 $i$ . 则 $y_i = 1$ ， $y_j =0,(j \\neq i)$ .\n反向求导为了进行反向传播，我们需要求得 $\\cfrac{\\partial L}{\\partial x_i}$ 的值，假设输入样本的类标签是 $t,t \\in [1,C]$ ,为了计算偏导数，需要分为两种情况：\n\n当 $ i = s$ 时：\n\n\\begin{align*}\n\\frac{\\partial L}{\\partial x_i} \n&= \\frac{\\partial ( - y_tln(\\frac{e^{x_t} }{\\sum\\limits_{j = 1}^C {e^{x_j}} }))}{\\partial x_t} \\\\\n&=  - y_t \\times \\frac{\\sum\\limits_{j = 1}^C e^{x_j} }{e^{x_t} } \\times \\frac{e^{x_t} \\times \\sum\\limits_{j = 1}^C e^{x_j}  - e^{2 x_t} } { { {(\\sum\\limits_{j = 1}^K e^{x_j} })}^2} \\\\\n&=  - y_t \\times \\frac{\\sum\\limits_{j = 1}^C e^{x_j}  - e^{x_t}}{\\sum\\limits_{j = 1}^K e^{x_j}} \\\\\n&=  - y_t(1 - {z_t})\n\\end{align*}\n当 $i \\neq s$ 时：\n\n\\begin{align*}\n\\frac{\\partial J}{\\partial x_i} \n&= \\frac{\\partial ( - {y_t}ln(\\frac{e^{x_t} }{\\sum\\limits_{j = 1}^C e^{x_j} }))} {\\partial {x_i} } \\\\\n&=  - {y_t} \\times \\frac{\\sum\\limits_{j = 1}^C e^{x_j} }{e^{x_t} } \\times \\frac{ - e^{x_t} \\times e^{x_i}} { {(\\sum\\limits_{j = 1}^C e^{x_j} )}^2} \\\\\n&=  {y_t}\\frac{e^{x_i}}{\\sum\\limits_{j = 1}^K e^{x_j} } \\\\\n&=  {y_t}{z_i}\n\\end{align*}\n\n将两种情况统一为一种情况：\n\n\\frac{\\partial J}{\\partial x_i}  = z_i - y_iLarge Margin Cosine LossLarge Margin Cosine Loss是论文CosFace中提出的一种softmax的改进方式，以下简记这个损失函数为LMCL。为了帮助理解，下面的符号将与论文中保持一致，可能会与上面softmax+crossEntropy部分稍有不同。\nMotivation由于论文任务是人脸的识别和验证，所以任务的目的就是训练一种可以代表某个人脸特征，而且要尽量使得类内方差更小，类间方差更大。作者认为原始的softmax交叉熵损失函数的区分性能力不够强，因为原始的softmax交叉熵只是为了在训练集上优化每个样本的分类结果，不同类之间的距离（特征的距离）可能并没有通过训练变得很大，这样的话，如果测试集的数据和训练集稍有不同，或对数据做稍稍的扰动，一个类的数据就可能被分类为另一个类。\n除此之外，softmax交叉熵优化的目标和测试的时候策略也稍有不同。如果我们将神经网络所提取的特征记为 $x$ ,假设向量 $x$ 的维度是 $F$. 网络所要进行的分类个数是 $C$ 。为了进行softmax交叉熵的计算，还需要一个维度为 $F \\times C$ 的矩阵 $W$将向量 $x$ 映射为 $f$. 这里 $f_j = W^T_jx = ||W_j|| \\, ||x||cos \\theta _j$ .而在测试的时候，往往会只求两个特征向量之间的cosine距离来判断两个向量之间的相似性，也就是说测试的时候只将 $cos \\theta$ 作为了评判标准。而在训练的过程中，最后损失函数的优化可能是通过对 $||W||$ 和 $||x||$ 的调整实现的。\n实现策略为了对以上两个缺陷进行改进，论文分别通过对损失函数加margin和对 $W$ 、$x$进行长度的normlize来实现。对损失函数加margin可以使得不同的类之间的距离更大，增加模型的鲁棒性，对$W$ 、$x$进行长度归一化可以将损失函数的优化目标聚焦到 $cos \\theta$ 上，按照论文中所说的就是去除了radial variation,即去除了$W$ 、$x$的模长对结果的影响。最后的改进过的损失函数被定义为：\n\nL_{lmc} = \\cfrac{1}{N}\\sum_i-log\n\\frac{e^{s(cos(\\theta_{y_i,i})-m)}}{e^{s(cos(\\theta_{y_i,i})-m)} + \n\\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})}} \\\\\nW = \\cfrac{W^*}{||W||} \\\\\nx = \\cfrac{x^*}{||x||} \\\\\ncos(\\theta_j,i) = W_j^Tx_i反向求导设：\n\nz_i  = \\frac{e^{s(cos(\\theta_{y_i,i})-m)}}{e^{s(cos(\\theta_{y_i,i})-m)} + \n\\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})}} \\; ,i = y_j\\\\\nz_i  = \\frac{e^{scos(\\theta_j,i)}}{e^{s(cos(\\theta_{y_i,i})-m)} + \n\\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})}} \\; ,i \\neq y_j与softmax交叉熵损失函数求导时一样仍需要两种情况：\n\n当 $ i = y_i $ 时：\n\n\n\n\n  \\begin{align*}\n  \\cfrac{\\partial L_{lmc}}{\\partial cos(\\theta_{y_i},i)} \n  &= -\\cfrac{e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})}}\n  {e^{s(cos(\\theta_{y_i,i})-m)}}\n  \\times \n  \\cfrac{se^{s(cos(\\theta_{y_i,i})-m)} \\times (e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})})\n   - se^{2s(cos(\\theta_{y_i,i})-m)}}\n  {(e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})})^2}\\\\\n  &= -\\cfrac{s \\times (e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})})\n   - se^{s(cos(\\theta_{y_i,i})-m)}}\n  {e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})}} \\\\\n  &= - (s - sz_i)\\\\\n  &= s(z_i-1)\n  \\end{align*}\n当 $ i \\neq y_i $ 时：\n\n\\begin{align*}\n\\cfrac{\\partial L_{lmc}} {\\partial cos(\\theta_j,i)} \n&=-\\cfrac{e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})}}\n  {e^{s(cos(\\theta_{y_i,i})-m)}}\n\\times \n\\cfrac{-se^{s(cos(\\theta_j,i))}*e^{s(cos(\\theta_{y_i,i})-m)}}\n{(e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})})^2}\\\\\n&= \\cfrac{se^{s(cos(\\theta_j,i))}}\n{e^{s(cos(\\theta_{y_i,i})-m)} + \\sum_{j \\neq y_i} e^{scos(\\theta_{j,i})}} \\\\\n&= sz_i\n\\end{align*}将两种情况统一为一种情况：\n\n\\cfrac{\\partial L_{lmc}}{\\partial cos(\\theta_j,i)}   = s(z_i - y_i)如论文中所说s是一个比较大的数，因此s会使得梯度变得更大，同时m的出现也会使得梯度变得更大，因此从公式推导来看，似乎在softmax交叉熵已经将梯度减小的一定程度已无法更新的时候，这个loss的梯度还会使得模型更新。\n\n\n","dateCreated":"2019-06-10T10:03:07+08:00","dateModified":"2019-10-26T15:48:10+08:00","datePublished":"2019-06-10T10:03:07+08:00","description":"损失函数作为优化一个神经网络的目标，对于神经网络最后的性能影响重大。Softmax+crossentropy作为一个分类的损失函数受到大家广泛的使用。本文将对于softmax做一个推导，并对很多论文中对softmax所做的改进做一个总结。","headline":"多种Softmax对比","image":["cover.jpg","cover.jpg"],"mainEntityOfPage":{"@type":"WebPage","@id":"http://czy97.github.io/2019/06/10/softmax/"},"publisher":{"@type":"Organization","name":"Zhengyang Chen","sameAs":["https://github.com/"],"image":"kelasong.jpeg","logo":{"@type":"ImageObject","url":"kelasong.jpeg"}},"url":"http://czy97.github.io/2019/06/10/softmax/","keywords":"LOSS","thumbnailUrl":"cover.jpg"}</script>
    <meta name="description" content="损失函数作为优化一个神经网络的目标，对于神经网络最后的性能影响重大。Softmax+crossentropy作为一个分类的损失函数受到大家广泛的使用。本文将对于softmax做一个推导，并对很多论文中对softmax所做的改进做一个总结。">
<meta name="keywords" content="javascript,hexo">
<meta property="og:type" content="blog">
<meta property="og:title" content="多种Softmax对比">
<meta property="og:url" content="http://czy97.github.io/2019/06/10/softmax/index.html">
<meta property="og:site_name" content="Zhengyang Chen&#39;s blog">
<meta property="og:description" content="损失函数作为优化一个神经网络的目标，对于神经网络最后的性能影响重大。Softmax+crossentropy作为一个分类的损失函数受到大家广泛的使用。本文将对于softmax做一个推导，并对很多论文中对softmax所做的改进做一个总结。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-10-26T07:48:10.026Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="多种Softmax对比">
<meta name="twitter:description" content="损失函数作为优化一个神经网络的目标，对于神经网络最后的性能影响重大。Softmax+crossentropy作为一个分类的损失函数受到大家广泛的使用。本文将对于softmax做一个推导，并对很多论文中对softmax所做的改进做一个总结。">
    
    
        
    
    
        <meta property="og:image" content="http://czy97.github.io/assets/images/kelasong.jpeg"/>
    
    
        <meta property="og:image" content="http://czy97.github.io/2019/06/10/softmax/cover.jpg"/>
        <meta class="swiftype" name="image" data-type="enum" content="http://czy97.github.io/2019/06/10/softmax/cover.jpg" />
    
    
        <meta property="og:image" content="http://czy97.github.io/2019/06/10/softmax/cover.jpg"/>
        <meta class="swiftype" name="image" data-type="enum" content="http://czy97.github.io/2019/06/10/softmax/cover.jpg" />
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-kpnqlflosyl4vwoz7sl207xaiamkqlxu50xxxjqplreb5zzvr9di4ar5sfvh.min.css">
    <!--STYLES END-->
    

    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Zhengyang Chen&#39;s blog</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
            <img class="header-picture" src="/assets/images/kelasong.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/kelasong.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Zhengyang Chen</h4>
                
                    <h5 class="sidebar-profile-bio"><p>No hurry</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--full"
             style="background-image:url('/2019/06/10/softmax/cover.jpg');"
             data-behavior="4">
            
                <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            多种Softmax对比
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2019-06-10T10:03:07+08:00">
	
		    6月 10, 2019
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Summary/">Summary</a>


    
</div>

    
</div>

            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>损失函数作为优化一个神经网络的目标，对于神经网络最后的性能影响重大。Softmax+crossentropy作为一个分类的损失函数受到大家广泛的使用。本文将对于softmax做一个推导，并对很多论文中对softmax所做的改进做一个总结。</p>
<a id="more"></a>
<ul>
<li><h1 id="Softmax-CrossEntropy损失函数"><a href="#Softmax-CrossEntropy损失函数" class="headerlink" title="Softmax+CrossEntropy损失函数"></a>Softmax+CrossEntropy损失函数</h1><h3 id="损失函数的定义"><a href="#损失函数的定义" class="headerlink" title="损失函数的定义"></a>损失函数的定义</h3><p>softmax交叉熵损失函数主要是针对多类的分类任务使用。假设任务中的类别数是 $C$ 。为了使用softmax交叉熵损失函数，首先需要将输入转为一个 $C$ 维的向量，这里用 $x_i ,(i \in [1,C])$ 来表示向量中每个位置的值。</p>
<p>然后，对这个$C$维向量中的所有值做softmax运算，以将每个值转为一种可以作为某种具有概率意义的值,计算之后的值记为 $z_i ,(i \in [1,C]) $ ,这样所有的$z_i$之和便是1，便可以用来表示输入样本输入每个类的概率值：</p>
<script type="math/tex; mode=display">
z_i =  \cfrac{e^{x_i}}{\sum_{j=1}^Ce^{x_j}}</script><p>最后对 $z_i$ 做negtive log likelihood和crossEntropy运算，最后的损失函数被定义为：</p>
<script type="math/tex; mode=display">
L =  -\sum^C_{i=1}y_iln(z_i) = -y_iln(z_i)</script><p>这里的 $y_i$ 输入样本的类标签，如果输入样本输入类 $i$ . 则 $y_i = 1$ ， $y_j =0,(j \neq i)$ .</p>
<h3 id="反向求导"><a href="#反向求导" class="headerlink" title="反向求导"></a>反向求导</h3><p>为了进行反向传播，我们需要求得 $\cfrac{\partial L}{\partial x_i}$ 的值，假设输入样本的类标签是 $t,t \in [1,C]$ ,为了计算偏导数，需要分为两种情况：</p>
<ul>
<li><p>当 $ i = s$ 时：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial L}{\partial x_i} 
&= \frac{\partial ( - y_tln(\frac{e^{x_t} }{\sum\limits_{j = 1}^C {e^{x_j}} }))}{\partial x_t} \\
&=  - y_t \times \frac{\sum\limits_{j = 1}^C e^{x_j} }{e^{x_t} } \times \frac{e^{x_t} \times \sum\limits_{j = 1}^C e^{x_j}  - e^{2 x_t} } { { {(\sum\limits_{j = 1}^K e^{x_j} })}^2} \\
&=  - y_t \times \frac{\sum\limits_{j = 1}^C e^{x_j}  - e^{x_t}}{\sum\limits_{j = 1}^K e^{x_j}} \\
&=  - y_t(1 - {z_t})
\end{align*}</script></li>
<li><p>当 $i \neq s$ 时：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial J}{\partial x_i} 
&= \frac{\partial ( - {y_t}ln(\frac{e^{x_t} }{\sum\limits_{j = 1}^C e^{x_j} }))} {\partial {x_i} } \\
&=  - {y_t} \times \frac{\sum\limits_{j = 1}^C e^{x_j} }{e^{x_t} } \times \frac{ - e^{x_t} \times e^{x_i}} { {(\sum\limits_{j = 1}^C e^{x_j} )}^2} \\
&=  {y_t}\frac{e^{x_i}}{\sum\limits_{j = 1}^K e^{x_j} } \\
&=  {y_t}{z_i}
\end{align*}</script></li>
</ul>
<p>将两种情况统一为一种情况：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial x_i}  = z_i - y_i</script><h1 id="Large-Margin-Cosine-Loss"><a href="#Large-Margin-Cosine-Loss" class="headerlink" title="Large Margin Cosine Loss"></a>Large Margin Cosine Loss</h1><p>Large Margin Cosine Loss是论文<a href="https://arxiv.org/pdf/1801.09414.pdf" target="_blank" rel="noopener">CosFace</a>中提出的一种softmax的改进方式，以下简记这个损失函数为LMCL。为了帮助理解，下面的符号将与论文中保持一致，可能会与上面softmax+crossEntropy部分稍有不同。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>由于论文任务是人脸的识别和验证，所以任务的目的就是训练一种可以代表某个人脸特征，而且要尽量使得<strong>类内方差更小，类间方差更大</strong>。作者认为原始的softmax交叉熵损失函数的<strong>区分性能力不够强</strong>，因为原始的softmax交叉熵只是为了在训练集上优化每个样本的分类结果，不同类之间的距离（特征的距离）可能并没有通过训练变得很大，这样的话，如果测试集的数据和训练集稍有不同，或对数据做稍稍的扰动，一个类的数据就可能被分类为另一个类。</p>
<p>除此之外，softmax交叉熵优化的目标和测试的时候策略也稍有不同。如果我们将神经网络所提取的特征记为 $x$ ,假设向量 $x$ 的维度是 $F$. 网络所要进行的分类个数是 $C$ 。为了进行softmax交叉熵的计算，还需要一个维度为 $F \times C$ 的矩阵 $W$将向量 $x$ 映射为 $f$. 这里 $f_j = W^T_jx = ||W_j|| \, ||x||cos \theta _j$ .而在测试的时候，往往会只求两个特征向量之间的<strong>cosine距离</strong>来判断两个向量之间的相似性，也就是说测试的时候只将 $cos \theta$ 作为了评判标准。而在训练的过程中，最后损失函数的优化可能是通过对 $||W||$ 和 $||x||$ 的调整实现的。</p>
<h3 id="实现策略"><a href="#实现策略" class="headerlink" title="实现策略"></a>实现策略</h3><p>为了对以上两个缺陷进行改进，论文分别通过对损失函数<strong>加margin</strong>和对 $W$ 、$x$进行<strong>长度的normlize</strong>来实现。对损失函数加margin可以使得不同的类之间的距离更大，增加模型的鲁棒性，对$W$ 、$x$进行长度归一化可以将损失函数的优化目标聚焦到 $cos \theta$ 上，按照论文中所说的就是去除了radial variation,即去除了$W$ 、$x$的模长对结果的影响。最后的改进过的损失函数被定义为：</p>
<script type="math/tex; mode=display">
L_{lmc} = \cfrac{1}{N}\sum_i-log
\frac{e^{s(cos(\theta_{y_i,i})-m)}}{e^{s(cos(\theta_{y_i,i})-m)} + 
\sum_{j \neq y_i} e^{scos(\theta_{j,i})}} \\
W = \cfrac{W^*}{||W||} \\
x = \cfrac{x^*}{||x||} \\
cos(\theta_j,i) = W_j^Tx_i</script><h3 id="反向求导-1"><a href="#反向求导-1" class="headerlink" title="反向求导"></a>反向求导</h3><p>设：</p>
<script type="math/tex; mode=display">
z_i  = \frac{e^{s(cos(\theta_{y_i,i})-m)}}{e^{s(cos(\theta_{y_i,i})-m)} + 
\sum_{j \neq y_i} e^{scos(\theta_{j,i})}} \; ,i = y_j\\
z_i  = \frac{e^{scos(\theta_j,i)}}{e^{s(cos(\theta_{y_i,i})-m)} + 
\sum_{j \neq y_i} e^{scos(\theta_{j,i})}} \; ,i \neq y_j</script><p>与softmax交叉熵损失函数求导时一样仍需要两种情况：</p>
<ul>
<li>当 $ i = y_i $ 时：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
  \begin{align*}
  \cfrac{\partial L_{lmc}}{\partial cos(\theta_{y_i},i)} 
  &= -\cfrac{e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})}}
  {e^{s(cos(\theta_{y_i,i})-m)}}
  \times 
  \cfrac{se^{s(cos(\theta_{y_i,i})-m)} \times (e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})})
   - se^{2s(cos(\theta_{y_i,i})-m)}}
  {(e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})})^2}\\
  &= -\cfrac{s \times (e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})})
   - se^{s(cos(\theta_{y_i,i})-m)}}
  {e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})}} \\
  &= - (s - sz_i)\\
  &= s(z_i-1)
  \end{align*}</script><ul>
<li><p>当 $ i \neq y_i $ 时：</p>
<script type="math/tex; mode=display">
\begin{align*}
\cfrac{\partial L_{lmc}} {\partial cos(\theta_j,i)} 
&=-\cfrac{e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})}}
  {e^{s(cos(\theta_{y_i,i})-m)}}
\times 
\cfrac{-se^{s(cos(\theta_j,i))}*e^{s(cos(\theta_{y_i,i})-m)}}
{(e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})})^2}\\
&= \cfrac{se^{s(cos(\theta_j,i))}}
{e^{s(cos(\theta_{y_i,i})-m)} + \sum_{j \neq y_i} e^{scos(\theta_{j,i})}} \\
&= sz_i
\end{align*}</script><p>将两种情况统一为一种情况：</p>
<script type="math/tex; mode=display">
\cfrac{\partial L_{lmc}}{\partial cos(\theta_j,i)}   = s(z_i - y_i)</script><p>如论文中所说s是一个比较大的数，因此s会使得梯度变得更大，同时m的出现也会使得梯度变得更大，因此从公式推导来看，似乎在softmax交叉熵已经将梯度减小的一定程度已无法更新的时候，这个loss的梯度还会使得模型更新。</p>
</li>
</ul>

            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-link" href="/tags/LOSS/">LOSS</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/06/10/kl/" data-tooltip="Kullback-Leibler Divergence Explained" aria-label="PREVIOUS: Kullback-Leibler Divergence Explained">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/05/10/gmm-fa/" data-tooltip="对于GMM和Factor Analysis的EM算法过程的直观理解" aria-label="NEXT: 对于GMM和Factor Analysis的EM算法过程的直观理解">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://czy97.github.io/2019/06/10/softmax/" title="Share on Facebook">
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://czy97.github.io/2019/06/10/softmax/" title="Share on Twitter">
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://czy97.github.io/2019/06/10/softmax/" title="Share on Google+">
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2019 Zhengyang Chen. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/06/10/kl/" data-tooltip="Kullback-Leibler Divergence Explained" aria-label="PREVIOUS: Kullback-Leibler Divergence Explained">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/05/10/gmm-fa/" data-tooltip="对于GMM和Factor Analysis的EM算法过程的直观理解" aria-label="NEXT: 对于GMM和Factor Analysis的EM算法过程的直观理解">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://czy97.github.io/2019/06/10/softmax/" title="Share on Facebook">
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://czy97.github.io/2019/06/10/softmax/" title="Share on Twitter">
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://czy97.github.io/2019/06/10/softmax/" title="Share on Google+">
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://czy97.github.io/2019/06/10/softmax/">
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=http://czy97.github.io/2019/06/10/softmax/">
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=http://czy97.github.io/2019/06/10/softmax/">
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/kelasong.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Zhengyang Chen</h4>
        
            <div id="about-card-bio"><p>No hurry</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>author.job</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Wu Han
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-1ssvxe9dejcm6dunmp9pwayfycay4apoppihhwx9lkyakwa6g5wjrerljwvl.min.js"></script>
<!--SCRIPTS END-->

    



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


</body>
</html>
